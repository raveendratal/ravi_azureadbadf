{"cells":[{"cell_type":"markdown","source":["### How to add partitionId in dataframe \n* __`Answer:`__ Using `spark_partition_id` function and withColumn we can get partitionid and add into dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7420b304-45fa-4e07-a28a-e35391b13b43"}}},{"cell_type":"code","source":["df_airlines = spark.read.csv(\"/databricks-datasets/asa/airlines\",header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96463dc5-37be-45b4-8a0b-e519a865fcdf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import spark_partition_id\ndf_airlines = df_airlines.withColumn(\"PART_ID\",spark_partition_id())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99d78ecb-af5f-4d56-9296-a0a5ff9c7ee9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### How to get row count by partitionId in dataframe \n* __`Answer:`__ Using `spark_partition_id` function and withColumn we can get partitionid and add into dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85dfdd77-d5dc-4b9f-a96e-9ff662d93d00"}}},{"cell_type":"code","source":["display(df_airlines.groupBy(\"PART_ID\").count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efcdfa90-5565-4da7-af0e-c4a2cad16e44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,7453215],[1,7009728],[2,7141922],[3,7140596],[4,7129270],[5,6488540],[6,5967780],[7,5683047],[8,5527884],[9,5411843],[10,5384721],[11,5351983],[12,5327435],[13,5271359],[14,5270893],[15,5180048],[16,5202096],[17,5092157],[18,5076925],[19,5070501],[20,6353026]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"PART_ID","type":"\"integer\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PART_ID</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>7453215</td></tr><tr><td>1</td><td>7009728</td></tr><tr><td>2</td><td>7141922</td></tr><tr><td>3</td><td>7140596</td></tr><tr><td>4</td><td>7129270</td></tr><tr><td>5</td><td>6488540</td></tr><tr><td>6</td><td>5967780</td></tr><tr><td>7</td><td>5683047</td></tr><tr><td>8</td><td>5527884</td></tr><tr><td>9</td><td>5411843</td></tr><tr><td>10</td><td>5384721</td></tr><tr><td>11</td><td>5351983</td></tr><tr><td>12</td><td>5327435</td></tr><tr><td>13</td><td>5271359</td></tr><tr><td>14</td><td>5270893</td></tr><tr><td>15</td><td>5180048</td></tr><tr><td>16</td><td>5202096</td></tr><tr><td>17</td><td>5092157</td></tr><tr><td>18</td><td>5076925</td></tr><tr><td>19</td><td>5070501</td></tr><tr><td>20</td><td>6353026</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"get partition wise record cound in pyspark dataframe","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":789392050839226}},"nbformat":4,"nbformat_minor":0}
